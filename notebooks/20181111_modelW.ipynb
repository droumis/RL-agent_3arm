{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W track modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- observation space is non-repeating two-well combinations of the 3 wells = 6 states \n",
    "- action space is the 3 wells \n",
    "- reward table would be the observation space x action space with values reinforcing alternation rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wells = ('A', 'B', 'C')\n",
    "outer_wells = ('A', 'C')\n",
    "home_well = ('B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space [('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B')]\n",
      "\n",
      "action space ('A', 'B', 'C')\n"
     ]
    }
   ],
   "source": [
    "# all possible two-well non-repeating combinations\n",
    "observation_space = list(itertools.permutations(all_wells, r=2))\n",
    "\n",
    "# the possible actions at any state\n",
    "action_space = all_wells\n",
    "\n",
    "print('observation space {}\\n'.format(observation_space))\n",
    "print('action space {}'.format(action_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outbound rewarded sequences [('A', 'B', 'C'), ('C', 'B', 'A')]\n",
      "\n",
      "inbound rewarded sequences [('A', 'C', 'B'), ('B', 'A', 'B'), ('B', 'C', 'B'), ('C', 'A', 'B')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get rewarded sequences to populate reward table\n",
    "sequences = list(itertools.product(all_wells, repeat=3))\n",
    "\n",
    "outbound_rewarded_sequences = []\n",
    "inbound_rewarded_sequences = []\n",
    "for seq in sequences:\n",
    "    if seq[0] in outer_wells and seq[2] in outer_wells and seq[0] != seq[2] and seq[1] in home_well:\n",
    "        outbound_rewarded_sequences.append(seq)\n",
    "    if seq[1] in outer_wells and seq[0] != seq[1] and seq[2] in home_well:\n",
    "        inbound_rewarded_sequences.append(seq)\n",
    "\n",
    "print('outbound rewarded sequences {}\\n'.format(outbound_rewarded_sequences))\n",
    "print('inbound rewarded sequences {}\\n'.format(inbound_rewarded_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reward_table():\n",
    "    reward_table = {}\n",
    "    for obsv in observation_space:\n",
    "        current_well = obsv[-1]\n",
    "        reward_table[obsv] = {}\n",
    "        for act in action_space:\n",
    "            reward_table[obsv][act] = {}\n",
    "            if act == current_well:\n",
    "                # if the action is the same as the most recent well, \n",
    "                # loop back obvs state and draw penalty\n",
    "                reward_table[obsv][act]['next_state'] = obsv\n",
    "            else:\n",
    "                reward_table[obsv][act]['next_state'] = tuple([current_well, act])\n",
    "            # set reward value\n",
    "            if obsv + tuple(act) in outbound_rewarded_sequences:\n",
    "                reward_table[obsv][act]['reward'] = 1\n",
    "            elif obsv + tuple(act) in inbound_rewarded_sequences:\n",
    "                reward_table[obsv][act]['reward'] = 1\n",
    "            else:\n",
    "                reward_table[obsv][act]['reward'] = 0\n",
    "    return reward_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_table = make_reward_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize q table to zeros\n",
    "def init_q_table():\n",
    "    q_table = {}\n",
    "    for obsv in observation_space:\n",
    "        q_table[obsv] = {}\n",
    "        for act in action_space:\n",
    "            q_table[obsv][act] = 0\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_q_table(q_table):\n",
    "    #quick check if q table, followed greedily without further updates, is consistent with W rules\n",
    "    q_table_max_sequences = [tuple(list(state) + list(max(actions, key=lambda act: actions[act]))) for state, actions in q_table.items()]\n",
    "#     print(q_table_max_sequences)\n",
    "    check = set(q_table_max_sequences).issubset(outbound_rewarded_sequences+inbound_rewarded_sequences)\n",
    "    if check:\n",
    "        print('q_table consistent with W-TRACK rules')\n",
    "    else:\n",
    "        print('q_table NOT consistent with W-TRACK rules')\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 892 µs, sys: 96 µs, total: 988 µs\n",
      "Wall time: 1.02 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "alpha = 0.4\n",
    "gamma = 0.7\n",
    "epsilon = 0.5\n",
    "\n",
    "q_table = init_q_table()\n",
    "state = random.choice(observation_space)\n",
    "performance_indicator_func = []\n",
    "\n",
    "for i in range(1, 100):\n",
    "    # act\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        # Explore action space\n",
    "        action = random.choice(all_wells)\n",
    "    else:\n",
    "        action = max(q_table[state], key=lambda act: q_table[state][act])\n",
    "    # gather\n",
    "    reward = reward_table[state][action]['reward']\n",
    "    old_qvalue = q_table[state][action]\n",
    "    next_state = reward_table[state][action]['next_state']\n",
    "    next_max_action = max(q_table[next_state], key=lambda act: q_table[next_state][act])\n",
    "    next_max_qvalue = q_table[next_state][next_max_action]\n",
    "    # update\n",
    "    new_value = (1 - alpha) * old_qvalue + alpha * (reward + gamma * next_max_qvalue)\n",
    "    q_table[state][action] = round(new_value,4)\n",
    "    # iterate\n",
    "    state = reward_table[state][action]['next_state']\n",
    "    #save\n",
    "    performance_indicator_func.append(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('A', 'B'): {'A': 1.2692, 'B': 1.253, 'C': 2.6036},\n",
       " ('A', 'C'): {'A': 0.6428, 'B': 1.5402, 'C': 0.4313},\n",
       " ('B', 'A'): {'A': 0.1823, 'B': 2.6246, 'C': 0.7329},\n",
       " ('B', 'C'): {'A': 0.3312, 'B': 2.6333, 'C': 1.3236},\n",
       " ('C', 'A'): {'A': 0, 'B': 1.9477, 'C': 0},\n",
       " ('C', 'B'): {'A': 2.5285, 'B': 0.8155, 'C': 0.6145}}"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_table consistent with W-TRACK rules\n"
     ]
    }
   ],
   "source": [
    "check_q_table(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This wasn't supposed to work!? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:reinforce]",
   "language": "python",
   "name": "conda-env-reinforce-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
